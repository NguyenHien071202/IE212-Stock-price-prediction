{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c68fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-G85UMP6C:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>kafka-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a00bd12f90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "scala_version = '2.12'  # your scala version\n",
    "spark_version = '3.0.1' # your spark version\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:2.8.0' #your kafka version\n",
    "]\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"kafka-example\").config(\"spark.jars.packages\", \",\".join(packages)).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362357cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'amzn_stock_data3'\n",
    "kafka_server = 'localhost:9092'\n",
    "\n",
    "kafkaDf = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\", topic_name).option(\"startingOffsets\", \"earliest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52fc479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1cda735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "| key|               value|           topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|NULL|[22 31 39 39 37 2...|amzn_stock_data3|        0|     0|2024-01-23 19:46:...|            0|\n",
      "|NULL|[22 31 39 39 37 2...|amzn_stock_data3|        0|     1|2024-01-23 19:46:...|            0|\n",
      "|NULL|[22 31 39 39 37 2...|amzn_stock_data3|        0|     2|2024-01-23 19:46:...|            0|\n",
      "|NULL|[22 31 39 39 37 2...|amzn_stock_data3|        0|     3|2024-01-23 19:46:...|            0|\n",
      "|NULL|[22 31 39 39 37 2...|amzn_stock_data3|        0|     4|2024-01-23 19:46:...|            0|\n",
      "|NULL|[22 31 39 39 37 2...|amzn_stock_data3|        0|     5|2024-01-23 19:46:...|            0|\n",
      "|NULL|[22 31 39 39 37 2...|amzn_stock_data3|        0|     6|2024-01-23 19:46:...|            0|\n",
      "+----+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafkaDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3d8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1ba084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafkaDf.selectExpr(\"CAST(value AS STRING)\").select(split(\"value\", \",\").alias(\"amzn_values\")) \\\n",
    "    .selectExpr(\"amzn_values[0] as Date\", \"amzn_values[1] as Open\", \\\n",
    "                \"amzn_values[2] as High\", \"amzn_values[3] as Low\", \\\n",
    "                \"amzn_values[4] as Close\",\"amzn_values[5] as Adj_close\",\\\n",
    "                \"amzn_values[6] as Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d676b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+--------+--------+---------+-----------+\n",
      "|       Date|    Open|    High|     Low|   Close|Adj_close|     Volume|\n",
      "+-----------+--------+--------+--------+--------+---------+-----------+\n",
      "|\"1997-05-15|0.121875|0.125000|0.096354|0.097917| 0.097917|1443120000\"|\n",
      "|\"1997-05-16|0.098438|0.098958|0.085417|0.086458| 0.086458| 294000000\"|\n",
      "|\"1997-05-19|0.088021|0.088542|0.081250|0.085417| 0.085417| 122136000\"|\n",
      "|\"1997-05-20|0.086458|0.087500|0.081771|0.081771| 0.081771| 109344000\"|\n",
      "|\"1997-05-21|0.081771|0.082292|0.068750|0.071354| 0.071354| 377064000\"|\n",
      "|\"1997-05-22|0.071875|0.072396|0.065625|0.069792| 0.069792| 235536000\"|\n",
      "|\"1997-05-23|0.070313|0.076042|0.066667|0.075000| 0.075000| 318744000\"|\n",
      "|\"1997-05-27|0.075521|0.082292|0.072917|0.079167| 0.079167| 173952000\"|\n",
      "+-----------+--------+--------+--------+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3c8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import from_json, split, current_date, year, col, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d276e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel=LinearRegressionModel.load(\"G:/Saved model 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb93a8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing live view refreshed every 5 seconds\n",
      "Seconds passed: 90\n",
      "+----------+--------------------+--------+-------------------+\n",
      "|      Date|            Features|   Close|         prediction|\n",
      "+----------+--------------------+--------+-------------------+\n",
      "|1997-05-15|[0.121875,0.125,0...|0.097917|0.11267927516808793|\n",
      "|1997-05-16|[0.098438,0.09895...|0.086458|0.09681569056116437|\n",
      "|1997-05-19|[0.088021,0.08854...|0.085417|0.09128152466000145|\n",
      "|1997-05-20|[0.086458,0.0875,...|0.081771|0.09178373546140325|\n",
      "|1997-05-21|[0.081771,0.08229...|0.071354|0.08015858364583206|\n",
      "|1997-05-22|[0.071875,0.07239...|0.069792| 0.0755507846551898|\n",
      "|1997-05-23|[0.070313,0.07604...|   0.075|0.08040359350605483|\n",
      "|1997-05-27|[0.075521,0.08229...|0.079167|0.08729854115335386|\n",
      "|1997-05-28|[0.08125,0.081771...|0.076563| 0.0861401671477983|\n",
      "|1997-05-29|[0.077083,0.07708...| 0.07526|0.08275572720081965|\n",
      "|1997-05-30|[0.075,0.075521,0...|   0.075|0.08273744019385013|\n",
      "|1997-06-02|[0.075521,0.07656...|0.075521|0.08410304526787288|\n",
      "|1997-06-03|[0.076563,0.07656...|0.073958|0.08264172070375456|\n",
      "|1997-06-04|[0.073958,0.07447...|0.070833|0.07925861865004152|\n",
      "|1997-06-05|[0.070833,0.07708...|0.077083|0.08258118174577715|\n",
      "|1997-06-06|[0.075781,0.08541...|0.082813|0.09179798192950683|\n",
      "|1997-06-09|[0.082813,0.08541...|0.084375|0.09311192506906646|\n",
      "|1997-06-10|[0.085417,0.08541...|0.079167|0.08661535711608255|\n",
      "|1997-06-11|[0.079688,0.08020...|0.077083|0.08579686771765538|\n",
      "|1997-06-12|[0.079167,0.08229...|0.080208|0.08868682794169784|\n",
      "+----------+--------------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "break\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 1000):\n",
    "    try:\n",
    "        print(\"Showing live view refreshed every 5 seconds\")\n",
    "        print(f\"Seconds passed: {x*5}\")\n",
    "        kafkaDf = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\", topic_name).option(\"startingOffsets\", \"earliest\").load()\n",
    "        df = kafkaDf.selectExpr(\"CAST(value AS STRING)\").select(split(\"value\", \",\").alias(\"amzn_values\")) \\\n",
    "    .selectExpr(\"amzn_values[0] as Date\", \"amzn_values[1] as Open\", \\\n",
    "                \"amzn_values[2] as High\", \"amzn_values[3] as Low\", \\\n",
    "                \"amzn_values[4] as Close\",\"amzn_values[5] as Adj_close\", \"amzn_values[6] as Volume\")\n",
    "        \n",
    "        df1 = df \\\n",
    "    .withColumn(\"Date\", expr(\"substring(Date, 2, 10)\")) \\\n",
    "    .withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"Open\", col(\"Open\").cast(\"double\")) \\\n",
    "    .withColumn(\"High\", col(\"High\").cast(\"double\")) \\\n",
    "    .withColumn(\"Low\", col(\"Low\").cast(\"double\")) \\\n",
    "    .withColumn(\"Close\", col(\"Close\").cast(\"double\")) \\\n",
    "    .withColumn(\"Adj_close\", col(\"Adj_close\").cast(\"double\")) \\\n",
    "    .withColumn(\"Volume\", expr(\"substring(Volume, 1, length(Volume)-1)\").cast(\"int\"))\n",
    "        \n",
    "        featureassembler=VectorAssembler(inputCols=[\"Open\", \"High\", \"Low\"], outputCol='Features')\n",
    "        output=featureassembler.transform(df1)\n",
    "        final_data=output.select(\"Date\", \"Features\", \"Close\")\n",
    "        final_output=lrModel.transform(final_data)\n",
    "        final_output.show()      \n",
    "        sleep(5)\n",
    "        clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"break\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98872910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
